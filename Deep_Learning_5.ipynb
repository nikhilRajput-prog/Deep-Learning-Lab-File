{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLfkKX3DkoWTY3fO1Uczlh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilRajput-prog/Deep-Learning-Lab-File/blob/main/Deep_Learning_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCPD3-8gPZbn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import re\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"poem.csv\")\n",
        "print(df.head())\n",
        "print(\"\\nTotal rows:\", len(df))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Mt25FUsPh6M",
        "outputId": "cb20db6b-996d-4073-885e-435ebc26958a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                text\n",
            "0  O my Luve's like a red, red rose\\nThatâ€™s newly...\n",
            "1  The rose is red,\\nThe violet's blue,\\nSugar is...\n",
            "2  How do I love thee? Let me count the ways.\\nI ...\n",
            "3  Had I the heavens' embroidered cloths,\\nEnwrou...\n",
            "4  I.\\n    Enough! we're tired, my heart and I.\\n...\n",
            "\n",
            "Total rows: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = []\n",
        "\n",
        "for line in df['text'].astype(str):\n",
        "    line = line.lower()\n",
        "    line = re.sub(r'[^a-z\\s]', '', line)\n",
        "    words.extend(line.split())\n",
        "\n",
        "vocab = sorted(set(words))\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "word2idx = {w:i for i,w in enumerate(vocab)}\n",
        "idx2word = {i:w for w,i in word2idx.items()}\n",
        "\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(\"Sample words:\", vocab[:10])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHqURe2SP_a4",
        "outputId": "8fedeeda-1225-47d1-da43-657f91bbed69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5439\n",
            "Sample words: ['a', 'abase', 'abased', 'abbeystones', 'abeyance', 'abide', 'abode', 'abodes', 'about', 'above']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_words = [word2idx[w] for w in words]\n",
        "\n",
        "print(\"First 20 encoded words:\")\n",
        "print(encoded_words[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM3YuXRQQUww",
        "outputId": "71edb43f-797e-4bb8-c4cf-3e6ad3fe8945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 encoded words:\n",
            "[3167, 3054, 2775, 2664, 0, 3748, 3748, 3897, 4726, 3109, 4407, 2351, 2473, 3167, 3054, 2775, 2664, 4727, 2896, 4726]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEQ_LEN = 5\n",
        "X, y = [], []\n",
        "\n",
        "for i in range(len(encoded_words) - SEQ_LEN):\n",
        "    X.append(encoded_words[i:i+SEQ_LEN])\n",
        "    y.append(encoded_words[i+SEQ_LEN])\n",
        "\n",
        "X = torch.tensor(X)\n",
        "y = torch.tensor(y)\n",
        "\n",
        "print(\"Input shape:\", X.shape)\n",
        "print(\"Target shape:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4p2l06mdQb2C",
        "outputId": "1642d5a4-8e8d-4c5c-e7c4-fca587b6df01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([24671, 5])\n",
            "Target shape: torch.Size([24671])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleRNN_Numpy:\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        self.Wxh = np.random.randn(hidden_size, vocab_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = np.zeros((self.Whh.shape[0], 1))\n",
        "        outputs = []\n",
        "        for x in inputs:\n",
        "            x = x.reshape(-1, 1)\n",
        "            h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
        "            y = self.Why @ h + self.by\n",
        "            outputs.append(y)\n",
        "        return outputs\n",
        "\n",
        "rnn_np = SimpleRNN_Numpy(vocab_size, hidden_size=32)\n",
        "print(\"NumPy RNN initialized\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tBxCdgwQmGL",
        "outputId": "78684882-a887-4053-903b-3383aa90bb49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy RNN initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_onehot = torch.zeros(X.size(0), SEQ_LEN, vocab_size)\n",
        "\n",
        "for i in range(X.size(0)):\n",
        "    for t in range(SEQ_LEN):\n",
        "        X_onehot[i, t, X[i, t]] = 1\n",
        "\n",
        "print(\"One-hot input shape:\", X_onehot.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8q8VlWPQwuK",
        "outputId": "028343d7-070d-4ade-8abc-5a5f54feca03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot input shape: torch.Size([24671, 5, 5439])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneHotRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(vocab_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "onehot_model = OneHotRNN(vocab_size, 128)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(onehot_model.parameters(), lr=0.003)\n",
        "\n",
        "print(onehot_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA-0VtFsQ5EH",
        "outputId": "1f0493c0-e9ef-47ce-f490-330debe0fac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OneHotRNN(\n",
            "  (rnn): RNN(5439, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=5439, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = onehot_model(X_onehot)\n",
        "    loss_onehot = criterion(outputs, y)\n",
        "    loss_onehot.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "onehot_time = time.time() - start_time\n"
      ],
      "metadata": {
        "id": "wMTHBDDEQ_Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final One-Hot Loss:\", loss_onehot.item())\n",
        "print(\"One-Hot Training Time:\", onehot_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfZxbh0wYVju",
        "outputId": "64be6bf9-1920-4c39-f4d4-a609c95ae9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final One-Hot Loss: 6.76938009262085\n",
            "One-Hot Training Time: 130.3256962299347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_onehot(start_word, length=15):\n",
        "    words = [start_word]\n",
        "    for _ in range(length):\n",
        "        inp = torch.zeros(1, SEQ_LEN, vocab_size)\n",
        "        for i, w in enumerate(words[-SEQ_LEN:]):\n",
        "            inp[0, i, word2idx[w]] = 1\n",
        "        out = onehot_model(inp)\n",
        "        next_word = idx2word[out.argmax().item()]\n",
        "        words.append(next_word)\n",
        "    return \" \".join(words)\n",
        "\n",
        "print(generate_onehot(\"love\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmqCGkC_RojC",
        "outputId": "97ea3cf0-a975-4686-e5ba-f9a2f14d506f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love the the the the the the the the the the the the the the the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        out, _ = self.rnn(x)\n",
        "        return self.fc(out[:, -1, :])\n",
        "\n",
        "embed_model = EmbeddingRNN(vocab_size, 100, 128)\n",
        "optimizer = torch.optim.Adam(embed_model.parameters(), lr=0.003)\n",
        "\n",
        "print(embed_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNbkANxCSaMy",
        "outputId": "e4e885b7-b0d1-4d7f-c29f-fd70a391448f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EmbeddingRNN(\n",
            "  (embedding): Embedding(5439, 100)\n",
            "  (rnn): RNN(100, 128, batch_first=True)\n",
            "  (fc): Linear(in_features=128, out_features=5439, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = embed_model(X)\n",
        "    loss_embed = criterion(outputs, y)\n",
        "    loss_embed.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "embed_time = time.time() - start_time\n",
        "\n",
        "print(\"Final Embedding Loss:\", loss_embed.item())\n",
        "print(\"Embedding Training Time:\", embed_time)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PD1dBPFSzH7",
        "outputId": "ed35f25b-078b-4516-c81b-752ca5f08c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Embedding Loss: 6.803170204162598\n",
            "Embedding Training Time: 47.6439049243927\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Final One-Hot Loss:\", loss_onehot.item())\n",
        "print(\"Final Embedding Loss:\", loss_embed.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DG34oYqXjbR",
        "outputId": "45a3f0db-926b-4e6a-ccda-b469f69cced4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final One-Hot Loss: 6.76938009262085\n",
            "Final Embedding Loss: 6.803170204162598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n========= COMPARISON SUMMARY =========\")\n",
        "print(f\"One-Hot Encoding  -> Loss: {loss_onehot.item():.4f}, Time: {onehot_time:.2f}s\")\n",
        "print(f\"Word Embeddings   -> Loss: {loss_embed.item():.4f}, Time: {embed_time:.2f}s\")\n",
        "\n",
        "if loss_embed.item() < loss_onehot.item():\n",
        "    print(\"Embedding model performs better based on loss.\")\n",
        "else:\n",
        "    print(\"One-Hot model performs better based on loss.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOwbVseQYelH",
        "outputId": "4b20fc59-3f79-4854-c284-70f1fc406b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========= COMPARISON SUMMARY =========\n",
            "One-Hot Encoding  -> Loss: 6.7694, Time: 130.33s\n",
            "Word Embeddings   -> Loss: 6.8032, Time: 47.64s\n",
            "One-Hot model performs better based on loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embedding(start_word, length=15):\n",
        "    words = [start_word]\n",
        "    for _ in range(length):\n",
        "        seq = torch.tensor([word2idx[w] for w in words[-SEQ_LEN:]]).unsqueeze(0)\n",
        "        out = embed_model(seq)\n",
        "        next_word = idx2word[out.argmax().item()]\n",
        "        words.append(next_word)\n",
        "    return \" \".join(words)\n",
        "\n",
        "print(generate_embedding(\"love\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxMRnF1fTG3I",
        "outputId": "17135424-6f39-4bde-c2f8-b4c28ed49488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love same and the and and and and and and and and and and and and\n"
          ]
        }
      ]
    }
  ]
}